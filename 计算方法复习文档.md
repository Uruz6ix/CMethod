<font size=7>**计算方法与人工智能复习文档**</font>
---
><font size=2 color=#87CEFA>*author: dwy
wechat: lesruchesmalades
email: yuruzan@hotmail.com
公众号：您是吃什么好的
有问题随时联系。*</font>
---
<font size=2>*本文档仅供参考，整理时间仓促，文档组织逻辑和老师给的提纲略有不同。
计算方法是非常有用的课程，所以本文档不是单纯面向期末考试，也希望能在今后的学习中~~给我自己~~做一个参考。pdf版本点击**目录**可以跳转到对应部分。~~觉得可以的话公众号给我打一瓶热带风味冰红茶钱吧，谢谢。~~*
*老师划重点把整本书都划了，我只能尽量~~能少一点是一点~~简明扼要。
排版很松，别打印了。~~打印了就不能想跳就跳，不得行。还有别点图，点了会进到我的图床里。~~
资料来自教材和互联网。*
</font>
##目录
[toc]

##第一章：绪论
###误差
有**绝对误差、相对误差、截断误差、舍入误差、模型误差**等误差，相关概念还有**有效数字**。
<font size=2 color=#aaaaaa>~~刚考完叽呜应该都还有点概念吧。~~</font>

比较不熟悉的几个是：
####截断误差
在设计算法时,经常要进行近似处理（如取Taylor展开的前有限项等），这样必然引入一定量级的误差。
####舍入误差
由于计算机的存储空间、字长是有限的,用浮点数近似表示实数，浮点运算要进行四舍五入处理所导致的误差。
###浮点数
C语言中的float和double型就是浮点数。用科学计数法表示的二进制小数，只要指数位和位底数位长度不超过浮点数定义的限制，可直接表示为浮点数：
$(-1)^{sign}(1.M)_2 \times 2^{E-1023} $。
[![7SQEMn.jpg](https://s4.ax1x.com/2022/01/06/7SQEMn.jpg)](https://imgtu.com/i/7SQEMn)
<font size=2 color=#aaaaaa>~~从互联网上毛来的图~~</font>

需要注意的几个点是：
* 避免相近两数相减
* 避免大数吃小数
* 避免除数远小于被除数的绝对值
* 减少运算次数
  
<font size=2 color=#aaaaaa>总之就是防止溢出和舍入误差，~~不信邪建议自己试一下，~~ 航c有几道关于浮点数的题，有余力的同学可以做一下增强理解，我没有。</font>

##第二章：非线性方程
###前置知识
####根
* 若$f(\alpha)=0,f'(\alpha)\neq0$，称$\alpha$为**单根**
* 若$f(\alpha)=f'(\alpha)=\cdots=f^{k-1}(\alpha)=0$，称$\alpha$为**k重根** 
####收敛速度
设序列$\{x_k\}$收敛于$x^*$，如存在正实数$p$使得$\lim\limits_{k\rightarrow\infin}\frac{|x^*-x_{k+1}|}{|x^*-x_k|^p}=C$，则称序列$\{x_k\}$收敛于$x^*$的**收敛速度**是$p$阶的。
###代数方程求根
####一元三次方程
三次方程的标准形为$x^3=px+q$，其解为：$$x=\sqrt[3]{{q\over2}-\sqrt{\left( {q\over2}\right)^2-\left({p\over3}\right)^3}}+\sqrt[3]{{q\over2}+\sqrt{\left( {q\over2}\right)^2-\left({p\over3}\right)^3}} $$
要注意的是，当$p$很小的时候，$q\over2$与$\sqrt{\left( {q\over2}\right)^2-\left({p\over3}\right)^3} $接近，需要引入修正来避免相近数相减。
####一元四次方程
<font size=2 color=#aaaaaa>~~依旧是从互联网毛来的图，大家感受一下，别记了，也记不住。谢谢b站up主名浮半生！~~ </font>
![image](https://i0.hdslb.com/bfs/article/204eef4e0eb8cf2109b8d413fc8a9af3e46bd816.jpg@942w_839h_progressive.webp)
四次方程的求根公式相当之长，因此只简单叙述一下思路。
通过线性变换，得到一元四次方程的标准形$x^4=px^2+qx+r$，两边配成完全平方形式：$$(x^2+a)^2=(p+2a)x^2+qx+r+a。$$当$$q^2 =4(p+2a)(r+a^2)$$时，右侧为完全平方式，利用一元三次方程求根公式求得$a$，再解一个关于$x$的一元二次线性方程即可。
###二分法
<font size=2 color=#aaaaaa>~~航c老朋友又见面了~~ </font>
一种根据闭区间套原理，收敛速度一阶的求根方法，需要范围两端异号，即$f(a)f(b)<0 $。
原理见下图：
[![7pLdnP.jpg](https://s4.ax1x.com/2022/01/07/7pLdnP.jpg)](https://imgtu.com/i/7pLdnP)
参考代码如下：

```c
//一个技巧：跟谁同号谁被替换掉
#include<stdio.h>
#include<math.h>
int main(){
    double e=1e-8;//误差
    double a=0,b=1;//范围
    double x;
while(fabs(b-a)>e){
    x=(a+b)/2;
    double f=exp(x)-2;//函数
    double fa=exp(a)-2;//也是函数
    double fb=exp(b)-2;//还是函数
    if(fabs(f)<e){
        break;
    }
    if((fa*f)>0){
    a=x;
    }
    else{
        b=x;
    }
}
printf("%lf",x);
return 0;
}
```
###定点法
将$f(x)=0$转化为$x=\phi(x)$，构造$x_{k+1}=\phi(x_k)$，并用$|x_{k+1}-x_k|<\epsilon$来控制迭代次数，其收敛速度是**一阶**的。
要是其收敛需要满足：
* 当$x\in[a,b]$时，$\phi (x)\in [a,b],$
* 对任意$x\in[a,b]$,存在$0\leq L<1$,使$|\phi'(x)|\leq L<1$
####Aitken加速
第一步迭代得到 $x₁ = \phi(x _{0(k)})$，再迭代第二次$x₂ = \phi(x₁)$，加权平均改进$x_{0(k+1)} = \frac{x₁ - (x₁ - x₂)²}{(x₁ - 2x₂ + x₀)}$判断是否符合精度要求,符合要求则结束迭代,否则继续。该方法在初始阶段非常有效，后期则会出现相近数相减的情况。
###牛顿法
利用泰勒展开得到：$$x_{k+1}=x_k-\frac{f(x_k)}{f'(x_k)}，$$将其作为迭代公式，即为牛顿法，其为**二阶收敛**。几何过程如图：
[![799QB9.md.jpg](https://s4.ax1x.com/2022/01/07/799QB9.md.jpg)](https://imgtu.com/i/799QB9)
其在范围$[a,b]$收敛到唯一根的条件为：
* $f(a)f(b)<0$，保证有根
* $f'(x)\neq0$，保证单调
* $f''(x)$不变号，保证曲线凹凸性不变
* $\left|\frac{f(a)}{f'(a)}\right|\leq b-a,\left|\frac{f(b)}{f'(b)}\right|\leq b-a$

需要注意的是，牛顿法的迭代次数要用两个因子来控制，即$f(x_k)<\epsilon_1$且$|x_k-x_{k+1}|<\epsilon_2$，否则会出现过早停止迭代的情况。

###牛顿下山法
牛顿法对初始值$x_0$要求较高，在牛顿法迭代公式中加入下山因子$\lambda$：$$x_{k+1}=x_k-\lambda\frac{f(x_k)}{f'(x_k)}$$，保证$|f(x_{k+1})|<|f(x_k)|$，放款初值，其它同牛顿法。
###弦截法
本质上就是用平均变化率代替牛顿法中导函数，其它同牛顿法，单点弦截迭代公式为：$$x_{k+1}=x_k-\frac{f(x_k)}{f(x_k)-f(x_0)}(x_k-x_0)$$
双点的差别不大。
###非线性方程组
<font size=2 color=#aaaaaa>~~为什么不试试神奇的python呢？~~</font>
和一元的非线性方程求解的思路一致，只是未知量从一元变为多元，用线性代数的思维可以理解成求解$F(x)=0$，其中$x=(x_1,x_2,\cdots,x_n)^T$，$F(x)=(f_1,f_2,\cdots,f_n)^T$,再类比一元非线性方程构造迭代公式进行求解。
有兴趣的同学可以参考[这篇博客](https://blog.csdn.net/wu_nan_nan/article/details/53335554?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164154672016780264065361%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164154672016780264065361&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-53335554.pc_search_insert_ulrmf&utm_term=%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%BB%84%E6%B1%82%E8%A7%A3&spm=1018.2226.3001.4187)，我觉得考试应该不会给这么大的计算量。
<font size=2 color=#aaaaaa>~~这写雅可比方程多是一件美事啊！~~</font>

##第三章：线性方程组
一整章教您怎么解线性方程$AX=B$
###线性代数知识补充
<font size=2 color=#aaaaaa>~~实不相瞒我大一线代只有91分，这个知识补充可能有很多谬误，不要打我。~~</font>
####范数
**向量的范数**是广义上的“长度”，是定义在$R^n$上的**实连续函数**，用$||X||$表示，~~眼熟的~~性质如下：
* $\parallel0\parallel=0$
* $\parallel-X\parallel=\parallel X\parallel$
* $\left|\parallel X\parallel-\parallel Y\parallel\right|\leq\parallel X-Y\parallel$
  
$L_p$范数即为$\parallel X\parallel_p=(\sum_{i=1}^{n}|x_i|^p)^{1\over p}$，值得注意的是$L_{\infin}$为$\parallel X\parallel _{\infin}={\max}_{1\leq i\leq n}|x_n|$，而0范数等于n.


$X$诱导的**矩阵范数**定义为$\parallel A\parallel=\sup_{\parallel X\parallel=1}\parallel AX\parallel$，其中$\parallel A\parallel_1=max_i\sum_{i=1}^n|a_{ij}|,\parallel A\parallel_\infin=max_j\sum_{j=1}^n|a_{ij}|$.

####条件数
条件数用于刻画方程组对误差的**敏感程度**，记为cond$(A)=\parallel A^{-1}\parallel\parallel A\parallel$，相对较大时，称方程为**病态的**，否则为**良态的**。
####对角占优
矩阵中每个**主对角元素**的模都大于与它同行的其他元素的模的总和，这种矩阵就叫**严格对角占优**的。
###高斯消元法
将方程组化为三角形（**消元**），再进行**回代**，~~没啥好说的，会解就行~~时间复杂度为$O(n^3)$对高阶方程的容忍度不高。
其中**列主元素**法在消元过程中适当调换行，将各方程中要消去的那个未知数的系数按绝对值最大的作主元素，使主元素在对角线上。~~对于计算机来说蛮鸡肋，意义不大。~~

###雅可比法
迭代法，算起来又快又好。
对于n阶方程组$$a_{i1}x_1+a_{i2}x_2+\cdots+a_{in}x_n=b_i,i=1,2,\cdots,n$$若系数矩阵非奇异，可以通过移项得到迭代方程：$$x_i^{(k+1)}={1\over a_{ii}}(b_i-\sum^n_{j=1,j\neq i}a_{ij}x_{j}^{(k)})$$
给定一个初值向量$X^{(0)}$，若$X^{(k)}$收敛于$X^*$，则$X^*$为方程的解。类似牛顿法，需要通过前后两代的差来控制迭代次数。
###高斯-赛德尔法
是对雅可比法的改进，用$x_i^{(k+1)}$及时替换掉$x_i^{(k)}$，即：
$$x_i^{(k+1)}={1\over a_{ii}}({b_i-\sum_{j=1}^{i-1}a_{ij}x_{j}^{(k+1)}-\sum^{n}_{j=i+1}a_{ij}x_j^{(k)}})$$
###收敛条件
若$A$为严格对角占优矩阵，则雅克比迭代法、高斯-赛德尔迭代法均收敛。

##第四章：插值
插值的思路是用~~各种各样并不~~简单的曲线近似替代复杂曲线。
[![796p24.md.jpg](https://s4.ax1x.com/2022/01/07/796p24.md.jpg)](https://imgtu.com/i/796p24)
###代数插值
给定插值节点，构造代数多项式形成$f(x)$的插值函数$P_n(x)=1\cdot{}a_0+a_1x+\cdots+a_nx^n$，由插值条件可得线性方程组：$$y_i=1\cdot a_0+a_1x_i+\cdots+a_ix_i^n,i=0,1,\cdots,n $$其解唯一确定。
<font size=2 color=#aaaaaa>~~真有人考场手算这个吗...~~</font>
###拉格朗日插值
拉格朗日不解代数插值的方程，而是通过**多个函数相加**得到一个插值函数。这里有一个[形象的推导](https://www.zhihu.com/question/58333118/answer/262507694)，感兴趣的同学可以点进去看一下。<font size=2 color=#aaaaaa>~~有点像数电里的选择器~~</font>
$$P_n(x)=\sum_{k=0}^{n}l_k(x)y_k, $$
其中$$l_k=\prod_{j=0,j\neq k}^{n}\frac{(x-x_j)}{(x_k-x_j)}。$$
得到的曲线和代数插值的是一条。
拉格朗日插值构造简单、计算方便，但和代数插值一样，高精度要求高次多项式，光滑性差。
####克罗内克符号
$$\delta_{kj}=\begin{cases}
    1, k=i\\
    0,k\neq i    
\end{cases}$$
###埃尔米特插值
埃尔米特插值除了拟合函数需要通过给定的点，还要求拟合的函数在给**定点的导数**也等于原函数在该点的导数，因此有$2(n+1)$个条件。令：$$H(x)=\sum^n_{i=0}h_i(x)f(x_i)+\sum^n_{i=0}g_i(x)f'(x_i) $$
构造的思路与拉格朗日多项式一致~~选择器~~：$$\begin{cases}
    h_i(x_j)=\delta_{kj}\\
    h'_i(x_j)=0
\end{cases}$$

$$\begin{cases}
    g_i(x_j)=0\\
    g'_i(x_j)=\delta_{kj}
\end{cases}$$
如果不限定使用代数多项式，埃尔米特插值函数有无穷多个，而限定的情况下只有一个。
###三次样条插值
代数多项式插值次数越高越容易振荡，次数低精度低，可采用分段低次插值，光滑又精确，例如三次样条插值。具体的推导可以看书第81页，讲得有点令人费解。
三次样条函数可表示为：$$s(x)=\begin{cases}
    s_1(x)=a_1+b_1x+c_1x^2+d_1x^3,x\in[x_0,x_1]\\
\    \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \dots\\
    s_n(x)=a_n+b_nx+c_nx^2+d_nx^3,x\in[x_n-1,x_n]
\end{cases} $$
设$m_i=s''_i(x),i=0,\cdots,n$，自然样条中定义$m_0=m_n=0$
解下图方程：
[![IEenb9.gif](https://z3.ax1x.com/2021/11/03/IEenb9.gif)](https://imgtu.com/i/IEenb9)
而：$a_i=y_i,b_i=\frac{y_{i+1}-y_i}{h_i}-{h_i\over 2}m_i-{h_i\over 6}(m_{i+1}-m_i),c_i={m_i\over 2},d_i=\frac{m_{i+1}-m_i}{6h_i} $

##第五章：逼近
###最小二乘法
<font size=2 color=#aaaaaa>~~高中没学过？我不信。~~</font>
最小二乘法的本质就是使得到的函数与数据点的偏差的平方和最小，即$\sum_{i=0}^n[f(x_i)-y_i]^2$最小。课本第89页进行了很严谨的推广。
不过把叽呜考的记下来应该就可以了。
####点集的内积
定义$f(x)$和$g(x)$关于点集$\{x_0,x_1,\cdots,x_n\}$和权因子$\omega_i>0(i=0,1,\cdots,n)$的内积为$(f,g)=\sum_{i=0}^m\omega_if(x_i)g(x_i)$。
###函数内积
将点集的内积进行推广可得$$(f,g)=\int_a^b \rho(x)f(x)g(x)\det x$$，其中$\rho(x)$为权函数，性质如下：
* 在范围内恒大于等于0
* 积分$\int_a^b|x|^n\rho(x) \det x$对于非负整数$n$成立
* 仅当$g(x)\equiv0$时，$\int _a^bg(x)\rho(x) \det x=0$

常用权函数有$\frac{1}{\sqrt{1-x^2}},e^{-x^2}$和最重要的$1$。
内积其他性质可以从向量内积推广得来。
###最佳平方逼近
**基**是一组线性无关的函数，下文提到的**张成**即为线性组合。
给定基张成的函数和一个权函数，如果$\int_a^b\rho(x)[f(x)-\phi(x)]^2 \det x$最小，则称$\phi(x)$为$f(x)$在该基和权函数下的最佳平方逼近。
推导过程见课本第93页，此处只给出求解矩阵：
$$\sum_{j=0}^n(\phi_i,\phi_k)a_j=(f,\phi_j),k=0,1,\cdots,n $$


##第六章：数值积分
###插值型求积分公式
由拉格朗日插值公式可得$$P_n(x)=\sum_{k=0}^{n}l_k(x)f(x_k), $$
其中$$l_k=\prod_{j=0,j\neq k}^{n}\frac{(x-x_j)}{(x_k-x_j)}。$$
求定积分近似值：$$I=\sum_{k=0}^{n}\left(\int_a^b l_k \ \det x\right) f(x_k)$$

####梯形求积公式
两个插值节点，得：$$\int^b_af(x) \ \det x\approx{1\over2}(b-a)[f(a)+f(b)] $$
####辛卜生求积公式
三个插值节点，得：$$\int^b_af(x) \ \det x\approx{b-a\over6}[f(a)+4f(c)+f(a)] $$
####复化梯形公式
取步长$h=\frac{b-a}{n}$将面积化成小块分别应用梯形公式求解。
$$T_n={h\over2}\left[f(a)+2\sum_{k=1}^{n-1}f(x_k)+f(b) \right] $$
<font size=2 color=#aaaaaa>~~好像回到了积分的推导了，不 忘 初 心~~</font>
利用下面这个公式递推变步长，就是:
####变步长积分法
$$T_{2n}={h\over2}\sum_{k=0}^{n-1}f\left({x_k+x_{k+1}\over2}\right)+{T_n\over2} $$
丢一个初值进行循环就行。
###误差
请复习数分的**积分中值定理**和**介值定理**，严谨推导在课本第106页。求得误差为：
$$-\frac{h^2(b-a)f''(\xi)}{12}$$
###收敛加速
基于最后两次迭代的结果外推：$$\tilde{T}=\frac{4T_{2n}-T_n}{3} $$
##第七章：非线性优化
优化就是找全局最优解，但是本章的方法很容易陷入**局部最优**，需要结合下一章的算法来看。
###前置知识
####梯度
记$\triangledown f(x)=\left(\frac{\partial f(x)}{\partial x_1},\frac{\partial f(x)}{\partial x_2},\cdots,\frac{\partial f(x)}{\partial x_n}\right) $为函数$f(x)$的梯度。
####极值必要条件
梯度得$0$。
<font size=2 color=#aaaaaa>~~数分内容哈。~~</font>
###最速下降法
负梯度方向是函数值下降最快的方向（即**最速下降方向**），因此我们沿着这个方向搜索。
打不动字了，从网上毛了张过程图：
![image](https://img-blog.csdnimg.cn/2019090821392274.png)
这就是最速下降法，其速度一开始相当快，越接近极值点速度越慢。~~跑过就知道这玩意收敛速度慢得夸张~~
###一维搜索
求每一次搜索的步长，下面是我的**黄金分割法**代码：
```c
double gs(double x ,double y ,double vx ,double vy){
    double a ,b ,lamda ,miu ;
    int k;a =0;b =1;lamda =omi*a +(1-omi)*b ;miu =a +b -lamda ;
    for(k=1;k<2000;k++){
        if(f(x+lamda*vx,y+lamda*vy)<f(x+miu*vx,y+miu*vy)){
            b =miu ;
            miu =lamda ;
            lamda =a +b -miu ;}
        else{
            a =lamda ;
            lamda =miu ;
            miu =a +b -lamda ;}
        if(fabs(a-b)<delta_gs ){
            lamda =(a +b)/2;
            return lamda;}
    }
}
```

###拉格朗日乘子法
大一下数分的内容，用于求解带简单约束的非线性优化问题，这个问题的一般形式为：
$$\begin{cases}
    \min f(x_1,x_2,\cdots,x_n)\\
    s.t.\  g(x_1,x_2,\cdots,x_n)=0
\end{cases}$$
通过引入拉格朗日乘子来加入约束条件，构造**拉格朗日函数**：
$$F(x_1,\cdots,x_n,\lambda)=f(x_1,\cdots,x_n)-\lambda g(x_1,\cdots,x_n) $$
求解下述方程：
$$\begin{cases}
    \frac{\partial F}{\partial x_1}=0\\
    \frac{\partial F}{\partial x_2}=0\\
    \cdots \\
    \frac{\partial F}{\partial \lambda}=g=0
\end{cases}$$
##第八章：启发式算法
###概念综述
模仿现实规律与经验设计的算法被称为**启发式算法**。这类算法可以得到全局最优解，但可靠性略差，**缺乏统一、坚实和完整的理论体系**，对**控制参数**的依赖很大，需要结合问题反复测试参数。
同时收敛条件也不易给出。
###<div id='1'>神经元模型</div>

神经元相当于**分类器**，即M-P模型，核心思想为：
* 输入是线性累加的
* 输出只有0和1

神经元模型由以下部分构成：
* 输入分量$x_i$
* 权值分量$w _i$
* 偏置$\theta$
* 激活函数$f$
* 输出$y$
  
![](https://img-blog.csdnimg.cn/20190104104222949.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rhbmdob25naGFuaGFvbGk=,size_16,color_FFFFFF,t_70)
###遗传算法
~~建议复习高中生物必修三~~
通过**编码**将优化问题的解转化遗传算法需要的编码（即染色体），再随机生成一定规模的**初始种群**开始操作：
* 评估每条染色体所对应个体的**适应度**

* 遵照适应度越高，**选择**概率越大的原则，从种群中选择两个个体作为父方和母方（常使用轮盘赌）

* 抽取父母双方的染色体，进行**交叉**，产生子代

* 对子代的染色体进行**变异**

* 重复2，3，4步骤，直到新种群的产生

[这是带代码求解旅行商问题的我的作业](https://bhpan.buaa.edu.cn/#/link/A5BF232C402D31E3E5FB0A5DE96665FA?gns=30D9010356404B5E9FE524593AE1D157%2FC934C0BC65EF496DA9A5AF74ACAE72E0%2F2C93B0029AE1416AAF2AB352EC972DA9)，可以看一下是怎么编程实现的。

遗传算法的收敛性受到**种群规模**、**选择操作**、**交叉概率**和**变异概率**的影响。

###粒子群算法
~~弱化版遗传算法~~
粒子群算法通过设计一种无质量的粒子，粒子仅具有两个属性：**速度**$v$和**位置**$p$，速度代表移动的快慢，位置代表移动的方向。每个粒子在搜索空间中单独的搜寻最优解$pBest$，并将个体极值与整个粒子群里的其他粒子共享（**当前全局最优解**$gBest$），粒子群中的所有粒子根据当前个体极值和当前全局最优解调整自己的速度和位置。

更新公式为：$$v=wv+c_1r_1(pBest-p)+c_2r_2(gBest-p)$$$$p=p+v $$其中$c$为学习因子，$w$为权重，$r$为$[0，1]$上的随机数。

###模拟退火算法
对局部搜索算法的扩展。
####Metropolis准则
在降温过程中（给定了**降温因数**），给定两个状态$x_{old}$和$x_{new}$，从前态转为次态的概率$p$满足：
$$p=\begin{cases}
    1, E_{new}\leq E_{old}\\
    e^{-\triangle E/kT},E_{new}>E_{old}
\end{cases}$$

系统状态对应当前解，在当前解的附近搜索新解，而用能量来判定解的好坏。

##第九章：~~so-called~~人工智能
这个章节没有纸质书，[这是课本链接](https://bhpan.buaa.edu.cn/#/link/CD7BF2F89EFFF9D44A3069EC4681E118?gns=30D9010356404B5E9FE524593AE1D157%2FC934C0BC65EF496DA9A5AF74ACAE72E0%2FA781185BBDFE48029F6C25A0A09D4C2D)。
好消息使今年不考向量机，我也看不懂。
###机器学习前置知识
**机器学习**是根据已知数据构建数学模型，从而实现对未知数据预测的方法，其数据所对应的客观对象，在机器学习中称为**样本**。
主要有**分类**（离散型）和**回归**（连续型）两大类任务，分为**监督式学习**和**非监督式学习**两大类算法。
####数据集
已知属性称为**特征**，各特征值组成特征向量，未知属性称为**标签**。
特征向量、标签的集合称为**数据集**。用于训练模型的数据集称为**训练集**，用于性能测试的数据集称为**测试集**。
训练集与测试集的比例一般为**8:2**。
<font size=2 color=#aaaaaa>~~为毛书上没有的概念往年题考了，不理解~~</font>
####泛化
**泛化**是指其模型对于新样本的预测（拟合）能力。
如果模型在训练集上的预测（拟合）误差大，则称模型**欠拟合**；如果模型在训练集上的误差小，而在测试集上的误差大，就称模型**过拟合**。

###K邻近算法
一种监督式学习算法。给定一个训练数据集,对新的输入实例,在训练数据集中找到与该实例**最近邻**（有多种距离判定方式）的K个实例,这K个实例的多数属于某类,就把该输入实例分类到这个类下。
鸢尾花分类就是一个典型例题。
[这是我的KNN鸢尾花分类作业](https://bhpan.buaa.edu.cn/#/link/AC49FBD1BC24A2A341885CC0BF05AA36?gns=30D9010356404B5E9FE524593AE1D157%2FC934C0BC65EF496DA9A5AF74ACAE72E0%2F8B220557D472495D885D1CDD612D654D)。

###K-means算法
一种非监督式学习算法。样本**只有特征向量没有标签**~~让机器自己分~~。指定类别的总数K，通过聚类算法，可以将数据集中的样本分成k个类别。
步骤如下：
* 根据给定的类别总数 K，随机选取中心$c^{(1)},0\leq l\leq K$
* 遍历Ω中的每个样本$x^i$，计算$x^i$到每个簇的中心$c^{(l)}$的距离， $x^i$到那个簇的中心距离最近就将$x^i$归为那个簇
* 遍历每个簇，计算该簇中所有样本的中心$p$，用中心$p$ 替换该簇的中心$c^{(l)}$
* 若所有簇的中心趋于稳定，则停机；否则将所有簇中的样本清空，转第二步。

<font size=2 color=#aaaaaa>~~都做过上机实验吧~~</font>

###线性回归
课本讲得相当绕，但其实挺简单的~~就是我debug一晚上de不出来哪里错了~~。
这里有一篇很好的[博客](https://blog.csdn.net/weixin_44177568/article/details/103318761?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164162072916780366512837%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164162072916780366512837&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_ulrmf~default-3-103318761.pc_search_insert_ulrmf&utm_term=%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92&spm=1018.2226.3001.4187)。
给定一个线性函数$h(x)=\theta_0+\theta_1x_1+\cdots+\theta_nx_n$，有损失函数$J(\theta)$。
通过学习优化（最小二乘法、最速下降法）使**损失函数**尽可能小。
###逻辑回归
####Sigmoid函数
这个可太该记住了：
$$s(t)=\frac{1}{1+e^{-t}}$$
其满足$s'(t)=s(t)(1-s(t))$。
####逻辑回归本身
构造函数$g(x)=s(h(x))$把线性回归映射到$[0,1]$范围内，再转换为逻辑值。
###感知器
####单层感知器
神经元的定义请看[第八章](#1)
由两层神经元组成的神经网络叫**单层感知器**，包含**输出层**和**输入层**。
####PLA算法
一种单层感知器，用超平面线性分割数据集。
用来求向量$W$（用于预测的向量），使得在已知的数据中机器做出的判断与现实完全相同。
当$X$为二维向量时，相当于在平面上画出一条直线将所有的点分成两部分.
可表示为：$$y=sgn(w_1x_1+w_2x_2+w_3)$$
最终都可以找到线性解。
这是一篇讲得很好的[博客](https://zhuanlan.zhihu.com/p/52018570)。
```c
//一个代码
//double w1=0,w2=0,w3=0;
for(i=1;;i++){
    int f=0;
    for(int j=0;j<2000;j++){
        if(g(x[j][0],x[j][1])!=y[j]){
            if(y[j]==1){
                w1+=x[j][0],w2+=x[j][1],w3++;
            }
            else{
                w1-=x[j][0],w2-=x[j][1],w3--;
            }
            f++;
        }
    }
    if(f==0){
        break;
    }
}
```
####多层感知器
在单层感知器输入输出层中加入了**隐藏层**。~~有点人工智能的意思了~~
##附：~~不靠谱~~往年题简要分析
基本上就是概念题和计算题，概念有考比较碎的可能性，看完这个有空过一下课本吧，我也是靠写这个文档来过课本。那些很复杂的一般不会出计算题，常见的就是雅可比法、高斯消元法、牛顿法和拉格朗日等等这些，可以考虑列式子之后用~~卡西欧~~计算器水答案，且答案占分不高，但是式子不会就是不会。比大多数课后作业简单，当然很多同学~~比如我~~难作业是拿python水过去的。
没有手写代码，启发式算法和人工智能我觉得能把逻辑记下来理清楚就可以了，不咋可能会出计算题。~~而且跑起来蛮好玩的，要不是考试我肯定老喜欢了为啥这个考试排在理论力学后面啊~~
可以找我来要题。

---
<font size=7>**教我理论力学！！！！**</font>